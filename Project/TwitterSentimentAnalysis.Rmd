---
title: "TwitterSentimentAnalysis"
author: "Nitish Neelagiri"
date: "April 13, 2016"
output: pdf_document
---

```{r}
setwd("~/") 
library(twitteR) 
library(wordcloud) 
library(RColorBrewer) 
library(plyr) 
library(ggplot2) 
library(httr) 
library(stringr) 
library(e1071) 
library(tm) 
library(RTextTools) 
library(utils) 
library(qdap) 
library(ggplot2) 
library(SnowballC) 
library(caret)

##Establishing a handshake with Twitter Application. oauth_endpoints("twitter")
api_key <- "XXXXXXXXXXXXX"
api_secret <- "XXXXXXXXXXXXXX"
access_token <- "XXXXXXXXXXXXXX"
access_token_secret <- "XXXXXXXXXXXXXXX"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
extractLimit <- 20000
tweets_raw <- searchTwitter('Lakers', n = extractLimit, since = '2015-01-01')
tweets_transformed <- do.call("rbind", lapply(tweets_raw, as.data.frame))
allTweets <- tweets_transformed[tweets_transformed$isRetweet == "FALSE",]

##Cleaning the tweet data : Remove punctuation, digits, urls, tab spaces, duplicates allTweets$text = gsub("[[:punct:]]", "", allTweets$text)
allTweets$text = gsub("[[:digit:]]", "", allTweets$text)
allTweets$text = gsub("http\\w+", "", allTweets$text)
allTweets$text = gsub("[ \t]{2,}", "", allTweets$text) allTweets$text = gsub("^\\s+|\\s+$", "", allTweets$text) allTweets$text = gsub("[^[:alnum:]///' ]", "", allTweets$text) 

##Please do not Include â€“ Commented ## Actual Code
##Extract the data from saved csv file allTweets <- read.csv("Final_Data.csv")
##Exclude stop words from the tweets
exclude=scan("New Text Document (3).txt", what="", sep="\n") stopwords=c(stopwords("en"), exclude)
##Method for Stem Completion
stemCompletion_mod <- function(x,dict=Corpus_copy) {
PlainTextDocument(stripWhitespace(paste(stemCompletion(unlist(strsplit(as.character(x)," ")),dictionary=dict),sep="", collapse=" ")))
}
##The team has faced issues converting the entire tweet data into a corpus and generating the document matrix. So, the team decided to generated the matrix, that will further be used in model building, in six phases, one for each data file.
allTweet1=read.csv("data1.csv")
twitterCorpus1 <- Corpus(VectorSource(allTweet1$text))
twitterCorpus1 <- tm_map(twitterCorpus1,removeWords,stopwords) twitterCorpus1 <- tm_map(twitterCorpus1, content_transformer(tolower)) Corpus_copy <- twitterCorpus1
twitterCorpus1 <- tm_map(twitterCorpus1, stemDocument) twitterCorpus1 <- lapply(twitterCorpus1, stemCompletion_mod) twitterCorpus1 <- as.VCorpus(twitterCorpus1)
#################################################################################### allTweet2=read.csv("data2.csv")
twitterCorpus2 <- Corpus(VectorSource(allTweet2$text))
twitterCorpus2 <- tm_map(twitterCorpus2,removeWords,stopwords) twitterCorpus2 <- tm_map(twitterCorpus2, content_transformer(tolower)) Corpus_copy <- twitterCorpus2
twitterCorpus2 <- tm_map(twitterCorpus2, stemDocument) twitterCorpus2 <- lapply(twitterCorpus2, stemCompletion_mod) twitterCorpus2 <- as.VCorpus(twitterCorpus2)
#################################################################################### ###############
allTweet3=read.csv("data3.csv")
twitterCorpus3 <- Corpus(VectorSource(allTweet3$text))
twitterCorpus3 <- tm_map(twitterCorpus3,removeWords,stopwords) twitterCorpus3 <- tm_map(twitterCorpus3, content_transformer(tolower)) Corpus_copy <- twitterCorpus3
twitterCorpus3 <- tm_map(twitterCorpus3, stemDocument) twitterCorpus3 <- lapply(twitterCorpus3, stemCompletion_mod) twitterCorpus3 <- as.VCorpus(twitterCorpus3)
#################################################################################### ###############
allTweet4=read.csv("data4.csv")
twitterCorpus4 <- Corpus(VectorSource(allTweet4$text))
twitterCorpus4 <- tm_map(twitterCorpus4,removeWords,stopwords)
twitterCorpus4 <- tm_map(twitterCorpus4, content_transformer(tolower))
Corpus_copy <- twitterCorpus4
twitterCorpus4 <- tm_map(twitterCorpus4, stemDocument)
twitterCorpus4 <- lapply(twitterCorpus4, stemCompletion_mod)
twitterCorpus4 <- as.VCorpus(twitterCorpus4) #################################################################################### ############
allTweet5=read.csv("data5.csv")
twitterCorpus5 <- Corpus(VectorSource(allTweet5$text))
twitterCorpus5 <- tm_map(twitterCorpus5,removeWords,stopwords)
twitterCorpus5 <- tm_map(twitterCorpus5, content_transformer(tolower))
Corpus_copy <- twitterCorpus5
twitterCorpus5 <- tm_map(twitterCorpus5, stemDocument)
twitterCorpus5 <- lapply(twitterCorpus5, stemCompletion_mod)
twitterCorpus5 <- as.VCorpus(twitterCorpus5) #################################################################################### ############
allTweet6=read.csv("data6.csv")
twitterCorpus6 <- Corpus(VectorSource(allTweet6$text))
twitterCorpus6 <- tm_map(twitterCorpus6,removeWords,stopwords)
twitterCorpus6 <- tm_map(twitterCorpus6, content_transformer(tolower))
Corpus_copy <- twitterCorpus6
twitterCorpus6 <- tm_map(twitterCorpus6, stemDocument)
twitterCorpus6 <- lapply(twitterCorpus6, stemCompletion_mod)
twitterCorpus6 <- as.VCorpus(twitterCorpus6) #################################################################################### ############
##Generating final corpus
corpus <- c(twitterCorpus1, twitterCorpus2, twitterCorpus3, twitterCorpus4, twitterCorpus5, twitterCorpus6)
#Creates Document Matrix
termDocumentMatrix <- TermDocumentMatrix(corpus, control=list(wordLengths=c(1,Inf))) transposeTDM <- t(termDocumentMatrix) #transpose
transposeTDM <- removeSparseTerms(transposeTDM, 0.999)
documentMatrix <- as.matrix(transposeTDM)
##Remove highly correlated features correlatedMatrix <- cor(documentMatrix) correlatedMatrix[upper.tri(correlatedMatrix)] <- 0 diag(correlatedMatrix) <- 0
documentMatrix <- documentMatrix[,!apply(correlatedMatrix,2,function(x) any(x > 0.5 | x < -0.5))]
#Generate training and testing data
train <- sample(nrow(documentMatrix), 70*nrow(documentMatrix)/100) trainData <- documentMatrix[train,]
testData <- documentMatrix[-train,]
response <- as.matrix(allTweets$response)
trainResponse <- response[train,]
testResponse <- response[-train,]
#Model Building
##SVM
model=svm(trainData,trainResponse,type = "C-classification", kernel = "polynomial",gamma = 0.1, cost = 10)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "polynomial",gamma = 0.1, cost = 100)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "polynomial",gamma = 1, cost = 100)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "polynomial",gamma = 10, cost = 100)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "linear",gamma = 10, cost = 100) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "linear",gamma = 100, cost = 100) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "linear",gamma = 1000, cost = 100)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "linear",gamma = 10000, cost = 10)
pred=predict(model, testData) table(predict=pred,truth=testResponse) mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "linear",gamma = 100000, cost = 10)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "linear",gamma = 100000, cost = 100)
pred=predict(model, testData)
confusionMatrix(pred, testResponse)
table(predict=pred,truth=testResponse) mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 10, cost = 100) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 100, cost = 100) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 1, cost = 100) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 0.1, cost = 100) pred=predict(model, testData)
table(predict=pred,truth=testResponse) mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 0.01, cost = 100)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 0.01, cost = 10) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 0.001, cost = 1) pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse)
model=svm(trainData,trainResponse,type = "C-classification", kernel = "radial",gamma = 0.0001, cost = 10)
pred=predict(model, testData)
table(predict=pred,truth=testResponse)
mean(pred==testResponse) confusionMatrix(pred, testResponse)
##Naive Bayes
model=naiveBayes(trainData,trainResponse,laplace = 0, threshold = 0.9, eps = 1) pred=predict(object = model,newdata = testData, type="raw")
response <- rep(NA, nrow(pred)) for(i in 1:length(response))
response[i] <- names(which(pred[i,] == max(pred[i,]))) table(response, testResponse)
mean(response[!is.na(response)]==testResponse[!is.na(response)])
##Using RTextTools
documentMatrix <- create_matrix(allTweets$text, language="english", removeNumbers=TRUE, stemWords = TRUE, removeSparseTerms = .998, removeStopwords = TRUE, minWordLength = 3, toLower = TRUE, removePunctuation = TRUE)
container <- create_container(documentMatrix, allTweets$response, trainSize=1:7839, testSize=7840:11198, virgin=FALSE)
##Model Building
SVM_TRAIN <- train_model(container,"SVM") GLMNET_TRAIN <- train_model(container,"GLMNET") MAXENT_TRAIN <- train_model(container,"MAXENT") SLDA_TRAIN <- train_model(container,"SLDA") BOOSTING_TRAIN <- train_model(container,"BOOSTING") BAGGING_TRAIN <- train_model(container,"BAGGING") RF_TRAIN <- train_model(container,"RF")
NNET_TRAIN <- train_model(container,"NNET") TREE_TRAIN <- train_model(container,"TREE")
##Classification
SVM_CLASSIFY <- classify_model(container, SVM_TRAIN) GLMNET_CLASSIFY <- classify_model(container, GLMNET_TRAIN) MAXENT_CLASSIFY <- classify_model(container, MAXENT_TRAIN) SLDA_CLASSIFY <- classify_model(container, SLDA_TRAIN) BOOSTING_CLASSIFY <- classify_model(container, BOOSTING_TRAIN) BAGGING_CLASSIFY <- classify_model(container, BAGGING_TRAIN) RF_CLASSIFY <- classify_model(container, RF_TRAIN) NNET_CLASSIFY <- classify_model(container, NNET_TRAIN) TREE_CLASSIFY <- classify_model(container, TREE_TRAIN)
##Cross Validation
SVM_PRED <- cross_validate(container, 4, "SVM") GLMNET_PRED <- cross_validate(container, 4, "GLMNET") MAXENT_PRED <- cross_validate(container, 4, "MAXENT")
SLDA_PRED <- cross_validate(container, 4, "SLDA") BAGGING_PRED <- cross_validate(container, 4, "BAGGING") BOOSTING_PRED <- cross_validate(container, 4, "BOOSTING") RF_PRED <- cross_validate(container, 4, "RF")
NNET_PRED <- cross_validate(container, 4, "NNET") TREE_PRED <- cross_validate(container, 4, "TREE")
##Word Cloud
documentMatrix <- as.matrix(documentMatrix)
sortMatrix <- sort(colSums(documentMatrix),decreasing=TRUE) wordCloudFrame <- data.frame(word = names(sortMatrix),freq=sortMatrix)
theme <- brewer.pal(8,"Dark2")
wordcloud(wordCloudFrame$word,wordCloudFrame$freq, scale=c(8,.3),min.freq=2,max.words=100, random.order=T, rot.per=.15, colors=theme, vfont=c("sans serif","plain"))
##Sentiment Plot
qplot(allTweets$response, xlab = "Sentiment of Tweets")
#From this we can observe that there was an overall positive sentiment in the tweets, but it is also important to note that many of the words are neutral. Thus, a good feature selection algorithm would help reduce terms that do not contribute to either positive or negative sentiment in a tweet.
##Testing with new Data. Create a corpus with the data using the aforementioned steps. Use below steps to create a test matrix and test the prediction accuracy.
tdmTest=TermDocumentMatrix(twitterCorpus4, control=list(wordLengths=c(1,Inf)))
mdtTest=t(tdmTest)
documentMatrixTest=as.matrix(mdtTest)
new=setdiff(colnames(documentMatrix), intersect(colnames(documentMatrix),colnames(documentMatrixTest))) xx=data.frame(documentMatrixTest[,intersect(colnames(documentMatrix),colnames(documentMatrixTest) )])
a=data.frame(matrix(0, nrow=nrow(xx), ncol=ncol(documentMatrix)-ncol(xx)))
names(a) <- new
new_dataset=cbind(a,xx)
new_resp=as.matrix(allTweet4$response) pred=predict(model,new_dataset) mean(pred==allTweet4$response)
##Pruning the Tree
prune.tweet <- prune.misclass(TREE_TRAIN, best = 3) plot(prune.tweet)
text(prune.tweet, pretty = 0)
plot(TREE_TRAIN)
text(TREE_TRAIN, pretty = 0)
prune.tweet <- prune.misclass(TREE_TRAIN, best = 5) plot(prune.tweet)
text(prune.tweet, pretty = 0)
TREE_CLASSIFY <- classify_model(container, prune.tweet) TREE_PRED <- cross_validate(container, 4, "TREE")
#For Cross Validation in SVM with varying parameters meth=c("polynomial","linear", "radial") gam=c(0.0001,0.01,1,10)
cos=c(1,10,100)
for (m in meth){ for (g in gam){ for (c in cos){
b=paste("for method=",m," gamma=",g,"cost=",c)
print(b)
SVM_CV <- cross_validate(container, 3, "SVM",method = "C-classification", kernel = m,cross
= g, cost = c) }
} }
```